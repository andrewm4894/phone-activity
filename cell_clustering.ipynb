{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What \"types\" of cell towers do we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will consider the question of if we can find different 'types' of cell towers based on usage activity.\n",
    "\n",
    "As part of this we will have a few things to figure out:\n",
    "\n",
    "* What feature representation will we use for each cell?\n",
    "* What clustering approach will we use?\n",
    "* How will we then try to make sense and interpret the clusters?\n",
    "\n",
    "We will touch on each of these considerations as we go. And like any DS project there are countless ways we could go about formulating the problem, below is an initial approach which no doubt could be iterated and improved on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and some settings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T23:28:00.950708Z",
     "start_time": "2017-11-20T23:28:00.617412Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import hdbscan\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geojson\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "from descartes import PolygonPatch\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set path to where data and files are\n",
    "MY_DIR = r'C:\\Users\\Andrew\\github\\mobile-phone-activity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in each csv file one by one and then append them all into __df_big__ and a smaller version of the full data __df_small__ (for experimentation or quick development)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T23:26:25.834147Z",
     "start_time": "2017-11-20T23:25:47.579409Z"
    }
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "\n",
    "# get file paths\n",
    "path = MY_DIR\n",
    "files = glob.glob(path + \"\\data\\sms-call-internet-mi-2013-11-*\")\n",
    "\n",
    "# empty list to store individual df's into \n",
    "list_ = []\n",
    "\n",
    "# loop through files\n",
    "for file_ in files:\n",
    "    df_tmp = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df_tmp)\n",
    "\n",
    "# concat all the df's into one\n",
    "df_big = pd.concat(list_)\n",
    "\n",
    "# make a small version of the full data for developing with\n",
    "df_small = df_big.sample(n = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will just rename some columns and print out some summary stats and info on our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:31:38.949472Z",
     "start_time": "2017-11-20T21:29:27.979529Z"
    }
   },
   "outputs": [],
   "source": [
    "# pick big or small df to work with\n",
    "#df = df_small.copy()\n",
    "df = df_big.copy()\n",
    "\n",
    "# clean up some col names\n",
    "df.rename(columns={'CellID':'cell_id'}, inplace=True)\n",
    "\n",
    "# ensure cell_id is a string (for count uniques in describe())\n",
    "df['cell_id'] = df['cell_id'].astype(str)\n",
    "\n",
    "# ensure datetime is correct format\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(df.shape)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out some info on the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:32:01.239217Z",
     "start_time": "2017-11-20T21:32:01.215192Z"
    }
   },
   "outputs": [],
   "source": [
    "# look at some info about df\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out summary stats for each column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:32:37.464389Z",
     "start_time": "2017-11-20T21:32:04.330632Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are safe to set missing data values to be 0.\n",
    "\n",
    "So we are assuming the absence of measurements for any given cell in a specific hour represents a lack of traffic as opposed to any issues with the data... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:32:58.894720Z",
     "start_time": "2017-11-20T21:32:51.641066Z"
    }
   },
   "outputs": [],
   "source": [
    "# get missing value percentages\n",
    "print(\"% Missing Values\")\n",
    "print(df.isnull().sum()/len(df)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...looks from above that there is indeed quite a bit of missing data here. Given the data is already aggregated up to cell and hour level i'd probably be asking questions on this a bit more. Is it really common enoungh for a cell to go a whole hour without any call's in or out?<br><br>Not much we can do about that here so we will take it at face value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:35:44.692186Z",
     "start_time": "2017-11-20T21:35:41.701340Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill all nulls with 0\n",
    "df.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:35:48.019719Z",
     "start_time": "2017-11-20T21:35:46.136641Z"
    }
   },
   "outputs": [],
   "source": [
    "# look at a sample of data and keep track of the df shape as we do stuff\n",
    "print(df.shape)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build some features to help us aggregate up from day and hour to \"weekday\" or \"weekend\" and \"time of day\" (night, day, or evening). \n",
    "\n",
    "This will help us form a representation that should capture certain way's we think the data might behave while also being as compact as possible so as to avoid having lots of features feeding into the clustering which would be subject to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions).\n",
    "\n",
    "This is one point where we are making a specific design decision, as always its one we could play around with and revisit as we iterate (e.g. why not daily level or slice the day into different time bins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:42:27.734917Z",
     "start_time": "2017-11-20T21:41:29.309015Z"
    }
   },
   "outputs": [],
   "source": [
    "# get if weekend or weekday from datetime\n",
    "df['weekday'] = np.where((df['datetime'].dt.dayofweek < 5) , 'weekday', 'weekend')\n",
    "\n",
    "# segment hours into times of day\n",
    "def timeofday(hour):\n",
    "    if hour <= 6:\n",
    "        return 'night'\n",
    "    elif hour <= 17:\n",
    "        return 'day'\n",
    "    elif hour <= 23:\n",
    "        return 'evening'\n",
    "df['timeofday'] = df['datetime'].dt.hour.apply(timeofday)\n",
    "\n",
    "# make a single key field to describe day and time of traffic data\n",
    "df['day_time'] = df['weekday'] + '_' + df['timeofday']\n",
    "\n",
    "# drop some columns we no longer need\n",
    "del df['datetime']\n",
    "del df['countrycode']\n",
    "del df['weekday']\n",
    "del df['timeofday']\n",
    "\n",
    "print(df.shape)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics as percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will transform our data to be percentages - this idea here is to help with the interpretation and it also acts as a sort of way to normalize all our data to be on a similar scale which is key for distance measures when clustering.\n",
    "\n",
    "Again, this is a design choice, assuming we are really interested in the bahaviour of the traffic in terms of its eb's and flows then it's reasonable to focus on percentages. However one trade off here is that any notion of the size of a cell in terms of the volume of traffic it handles is no longer possible.  \n",
    "\n",
    "So we will first get the total values for each metric across the whole week and then we will use that to transform into percentages - so for example \"callsin\" will no become the % of all traffic in the sample period for that specific cell that falls on that specific \"day_time\" slice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:48:18.725340Z",
     "start_time": "2017-11-20T21:48:07.243334Z"
    }
   },
   "outputs": [],
   "source": [
    "# get cell totals for the whole week\n",
    "df_cell_totals = df.groupby(['cell_id'], as_index=False).sum()\n",
    "\n",
    "# get overall volumes regardless of direction\n",
    "df_cell_totals['calltotal'] = abs(df_cell_totals['callin']) + abs(df_cell_totals['callout'])\n",
    "df_cell_totals['smstotal'] = abs(df_cell_totals['smsin']) + abs(df_cell_totals['smsout'])\n",
    "df_cell_totals['internettotal'] = df_cell_totals['internet']\n",
    "\n",
    "print(df_cell_totals.shape)\n",
    "df_cell_totals.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will aggregate up the low level data to cell and day_time level - summing all metrics as we go..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:49:22.082092Z",
     "start_time": "2017-11-20T21:49:13.220477Z"
    }
   },
   "outputs": [],
   "source": [
    "# sum up to day_time, cell level\n",
    "df = df.groupby(['day_time','cell_id'], as_index=False).sum()\n",
    "\n",
    "print(df.shape)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create __df_pct__ dataframe where the metrics have all been transformed into percentages of cell total traffic for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:50:29.793528Z",
     "start_time": "2017-11-20T21:50:29.497159Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge in cell totals\n",
    "df_pct = pd.merge(df,df_cell_totals[['cell_id','internettotal','calltotal','smstotal']],how='left', on='cell_id')\n",
    "\n",
    "# fill all nulls with 0\n",
    "df_pct.fillna(value=0, inplace=True)\n",
    "\n",
    "# convert measures to percentages of totals for that cell for the sample\n",
    "df_pct['smsin'] = df_pct['smsin']/df_pct['smstotal']\n",
    "df_pct['smsout'] = df_pct['smsout']/df_pct['smstotal']\n",
    "df_pct['callin'] = df_pct['callin']/df_pct['calltotal']\n",
    "df_pct['callout'] = df_pct['callout']/df_pct['calltotal']\n",
    "df_pct['internet'] = df_pct['internet']/df_pct['internettotal']\n",
    "\n",
    "# fill all nulls with 0\n",
    "df_pct.fillna(value=0, inplace=True)\n",
    "\n",
    "print(df_pct.shape)\n",
    "df_pct.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:51:10.752557Z",
     "start_time": "2017-11-20T21:51:10.458853Z"
    }
   },
   "outputs": [],
   "source": [
    "# summary stats\n",
    "df_pct.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long to wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are interested in clustering cell towers we need to transform our data to be one row per cell. This is typically called moving from a long format to a wide format whereby we will pivot our data so make more columns for each type of row relating to a cell.\n",
    "\n",
    "So really we want to make our data wider with groups of metrics for each day_time value for each cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:56:43.348684Z",
     "start_time": "2017-11-20T21:56:43.010371Z"
    }
   },
   "outputs": [],
   "source": [
    "# now pivot to go wide with metrics for each day_time and one row per cell\n",
    "df_wide = df_pct[['cell_id',\n",
    "                  'day_time',\n",
    "                  'smsin',\n",
    "                  'smsout',\n",
    "                  'callin',\n",
    "                  'callout',\n",
    "                  'internet']].pivot(index='cell_id', columns='day_time')\n",
    "\n",
    "# fill in nulls\n",
    "df_wide.fillna(0, inplace=True)\n",
    "\n",
    "print(df_wide.shape)\n",
    "df_wide.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...We now have a hierarchial column index after our pivot - to make things easier we will flatten this and rename columns to be more meaningful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:58:03.313760Z",
     "start_time": "2017-11-20T21:58:03.156633Z"
    }
   },
   "outputs": [],
   "source": [
    "# get new column names so we can flatten the nested index\n",
    "new_col_names = []\n",
    "\n",
    "# loop through each level and build up new col names\n",
    "for c1 in df_wide.columns.levels[0]:\n",
    "    for c2 in df_wide.columns.levels[1]:\n",
    "        new_col_name = c1 + '_' + c2\n",
    "        new_col_names.append(new_col_name)\n",
    "\n",
    "#print(new_col_names)\n",
    "df_wide.columns = df_wide.columns.droplevel()\n",
    "df_wide.columns = new_col_names\n",
    "\n",
    "print(df_wide.shape)\n",
    "df_wide.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...note: we now have 10,000 rows of data as hoped - one per cell... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:59:04.682292Z",
     "start_time": "2017-11-20T21:59:04.659263Z"
    }
   },
   "outputs": [],
   "source": [
    "df_wide.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T21:59:07.436980Z",
     "start_time": "2017-11-20T21:59:07.061640Z"
    }
   },
   "outputs": [],
   "source": [
    "df_wide.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to perform our clustering and explore the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:00:32.622862Z",
     "start_time": "2017-11-20T22:00:32.531195Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a new df to use from here on\n",
    "df_final = df_wide.copy()\n",
    "\n",
    "print(df_final.shape)\n",
    "df_final.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will just start simple with a k-means approach. We have iterated on a few different choices of k and settled on a final value. \n",
    "\n",
    "We could use someting a bit more fancy like [HDBSCAN](http://hdbscan.readthedocs.io/en/latest/) which can fit more flexible cluster's and have a bit more of a data driven approach to the number of clusters. It also could be useful for helping pick out outlier cells - those for which no cluster was found to be close enough. \n",
    "\n",
    "But to keep it simple for now we will stick with k-means..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:04:27.424897Z",
     "start_time": "2017-11-20T22:04:27.408886Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define clustering approach\n",
    "#clusterer = hdbscan.HDBSCAN()\n",
    "clusterer = KMeans(n_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:04:52.531692Z",
     "start_time": "2017-11-20T22:04:49.281118Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do the clustering\n",
    "clusterer.fit(df_final)\n",
    "print(clusterer.labels_.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:05:15.758092Z",
     "start_time": "2017-11-20T22:05:15.746107Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the cluster labels back onto the input data\n",
    "df_final['cluster'] = clusterer.labels_\n",
    "#df_final['cluster_prob'] = clusterer.probabilities_ # onlt relvant for HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:05:24.893627Z",
     "start_time": "2017-11-20T22:05:24.788537Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_final.shape)\n",
    "df_final.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the amount of cells in each cluster to get a feel for how cells have been distributed to each cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:06:07.862978Z",
     "start_time": "2017-11-20T22:06:07.839956Z"
    }
   },
   "outputs": [],
   "source": [
    "# looks at % in each cluster\n",
    "cluster_counts = df_final['cluster'].value_counts()\n",
    "cluster_percents = cluster_counts / len(df_final['cluster'])\n",
    "print(cluster_counts)\n",
    "print(cluster_percents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do some plots to help us explore and understand each cluster as well as get some feel for what the main differences between the clusters are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:10:44.325398Z",
     "start_time": "2017-11-20T22:09:46.863527Z"
    }
   },
   "outputs": [],
   "source": [
    "# for each input metric plot a boxplot and violin plot to get a feel for how distributions differ by cluster\n",
    "for var in df_final.columns[0:30]:\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(12,6))\n",
    "    ax1.set_title(var)\n",
    "    ax2.set_title(var)\n",
    "    sns.boxplot(x=\"cluster\", y=var, data=df_final, ax=ax1)\n",
    "    sns.violinplot(x=\"cluster\", y=var, data=df_final, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...from the above boxplots we can get a feel for the variables that differ a lot accors clusters and those that don't..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:14:46.881915Z",
     "start_time": "2017-11-20T22:13:33.029992Z"
    }
   },
   "outputs": [],
   "source": [
    "# for each input metric plot a kernel density plot to get a feel for how distributions differ by cluster\n",
    "for var in df_final.columns[0:30]:\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    labels = []\n",
    "    for key, grp in df_final.groupby(['cluster']):\n",
    "        ax = grp.plot(ax=ax, kind='kde', x='cluster', y=var)\n",
    "        labels.append(key)\n",
    "    lines, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(lines, labels, loc='best')\n",
    "    ax.set_title(var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...similar to the box plots the density plots help us get a feel for the variables on which the clusters differ the most or least.\n",
    "\n",
    "Next we will try and take a more hollistic view and make some point plots for each cluster for each metric. This is a nice quick way to see where some of the main differences in each cluster are. \n",
    "\n",
    "To make this plot work however we have to summarise each metric in each cluster. So we will start with the median values of each metric per cluster and then also look at metrics like the mean and different quantile thresholds (to get a feel for the spread)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:18:52.221732Z",
     "start_time": "2017-11-20T22:18:46.112265Z"
    }
   },
   "outputs": [],
   "source": [
    "# get a long df giving the median value for each metric for each cluster\n",
    "df_long = pd.melt(df_final.groupby('cluster', as_index=False).median(), id_vars=['cluster'])\n",
    "agg_mode = 'Median'\n",
    "title = 'Median Values of Input Features By Cluster'\n",
    "plt.figure(figsize=(20,5))\n",
    "ax = sns.pointplot(x=\"variable\", y=\"value\", hue=\"cluster\", data=df_long, markers='o', linestyles='-')\n",
    "ax.set_ylabel(agg_mode)\n",
    "ax.set_title(title)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...so for example, from the above plot we can see that the main metric the clusters differ on is \"internet_weekday_day\" ranging from around 30% to 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:20:50.842601Z",
     "start_time": "2017-11-20T22:20:45.105271Z"
    }
   },
   "outputs": [],
   "source": [
    "# same as above but look at averages\n",
    "df_long = pd.melt(df_final.groupby('cluster', as_index=False).mean(), id_vars=['cluster'])\n",
    "agg_mode = 'Mean'\n",
    "title = 'Mean Values of Input Features By Cluster'\n",
    "plt.figure(figsize=(20,5))\n",
    "ax = sns.pointplot(x=\"variable\", y=\"value\", hue=\"cluster\", data=df_long, markers='o', linestyles='-')\n",
    "ax.set_ylabel(agg_mode)\n",
    "ax.set_title(title)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:21:26.870812Z",
     "start_time": "2017-11-20T22:21:20.510306Z"
    }
   },
   "outputs": [],
   "source": [
    "# 25th percentile\n",
    "df_long = pd.melt(df_final.groupby('cluster', as_index=False).quantile(.25), id_vars=['cluster'])\n",
    "df_long.columns = ['cluster','variable','value']\n",
    "agg_mode = '25th Percentile'\n",
    "title = '25th Percentile Values of Input Features By Cluster'\n",
    "plt.figure(figsize=(20,5))\n",
    "ax = sns.pointplot(x=\"variable\", y=\"value\", hue=\"cluster\", data=df_long, markers='o', linestyles='-')\n",
    "ax.set_ylabel(agg_mode)\n",
    "ax.set_title(title)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:21:46.352313Z",
     "start_time": "2017-11-20T22:21:40.178520Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 75th percentile\n",
    "df_long = pd.melt(df_final.groupby('cluster', as_index=False).quantile(.75), id_vars=['cluster'])\n",
    "df_long.columns = ['cluster','variable','value']\n",
    "agg_mode = '75th Percentile'\n",
    "title = '75th Percentile Values of Input Features By Cluster'\n",
    "plt.figure(figsize=(20,5))\n",
    "ax = sns.pointplot(x=\"variable\", y=\"value\", hue=\"cluster\", data=df_long, markers='o', linestyles='-')\n",
    "ax.set_ylabel(agg_mode)\n",
    "ax.set_title(title)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at summary stats for each cluster visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets look at each cluster on its own and plot similar metrics but just for one cluster at a time so we can get a feel for the range of values within each cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:23:50.796786Z",
     "start_time": "2017-11-20T22:23:50.374705Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make lots of long summary stat tables for each metric and cluster\n",
    "df_long_10 = pd.melt(df_final.groupby('cluster', as_index=False).quantile(.1), id_vars=['cluster'])\n",
    "df_long_10.columns = ['cluster','variable','value']\n",
    "df_long_10['metric'] = '10th Percentile'\n",
    "df_long_25 = pd.melt(df_final.groupby('cluster', as_index=False).quantile(.25), id_vars=['cluster'])\n",
    "df_long_25.columns = ['cluster','variable','value']\n",
    "df_long_25['metric'] = '25th Percentile'\n",
    "df_long_50 = pd.melt(df_final.groupby('cluster', as_index=False).quantile(.50), id_vars=['cluster'])\n",
    "df_long_50.columns = ['cluster','variable','value']\n",
    "df_long_50['metric'] = '50th Percentile'\n",
    "df_long_75 = pd.melt(df_final.groupby('cluster', as_index=False).quantile(.75), id_vars=['cluster'])\n",
    "df_long_75.columns = ['cluster','variable','value']\n",
    "df_long_75['metric'] = '75th Percentile'\n",
    "df_long_90 = pd.melt(df_final.groupby('cluster', as_index=False).quantile(.75), id_vars=['cluster'])\n",
    "df_long_90.columns = ['cluster','variable','value']\n",
    "df_long_90['metric'] = '90th Percentile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:24:34.015668Z",
     "start_time": "2017-11-20T22:24:33.962588Z"
    }
   },
   "outputs": [],
   "source": [
    "# union all the stats data together\n",
    "df_long_stats = df_long_10.append([df_long_25,df_long_50,df_long_75,df_long_90])\n",
    "\n",
    "print(df_long_stats.shape)\n",
    "df_long_stats.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:39:13.296635Z",
     "start_time": "2017-11-20T22:25:10.110640Z"
    }
   },
   "outputs": [],
   "source": [
    "# now loop through each cluster and plot the summary stats for each metric\n",
    "for c in clusterer.labels_: \n",
    "    df_long = df_long_stats[(df_long_stats.cluster == c)]\n",
    "    agg_mode = 'Summary Metric Value'\n",
    "    title = 'Summary Stats For Cluster ' + str(c)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    ax = sns.pointplot(x=\"variable\", y=\"value\", hue=\"metric\", data=df_long, markers='o', linestyles='-')\n",
    "    ax.set_ylabel(agg_mode)\n",
    "    ax.set_title(title)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...from the above plots we can get a sense for the variance within each metric for each cluster by seeing the spread of the different percentile lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Clusters On Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will plot the cluster on the actual cell grid map to see if any interesting geo spatial patterns jump out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:42:03.122432Z",
     "start_time": "2017-11-20T22:42:00.745475Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in the geo json data\n",
    "with open(MY_DIR + \"\\\\milano-grid.geojson\") as json_file:\n",
    "    json_data = geojson.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:44:56.340035Z",
     "start_time": "2017-11-20T22:43:35.368042Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure() \n",
    "ax = fig.gca() \n",
    "\n",
    "# some things to handle the coloring\n",
    "jet = cm = plt.get_cmap('Paired') \n",
    "cNorm  = colors.Normalize(vmin=0, vmax=clusterer.labels_.max())\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)\n",
    "\n",
    "# go through each cell and color it by cluster\n",
    "for i in range(1,len(json_data.features)):\n",
    "    \n",
    "    # put a try block in in case can't find some cells in the cluster data for some reason.\n",
    "    \n",
    "    try:\n",
    "        poly = json_data.features[i]['geometry']\n",
    "        cell_id = str(json_data.features[i]['properties']['cellId'])\n",
    "        cluster = df_final.loc[cell_id]['cluster']\n",
    "        colorVal = scalarMap.to_rgba(cluster)\n",
    "        \n",
    "        # add patch to the map\n",
    "        ax.add_patch(PolygonPatch(poly, fc=colorVal, ec=colorVal, alpha=0.8, zorder=1 ))\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ax.axis('scaled')\n",
    "\n",
    "fig.set_size_inches(11,11)\n",
    "plt.title(\"Cell Map Of Cluster Membership\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting thing about this map is that there does indeed seem to be spatial patterns here with nearby cells tending to be in the same cluster and 'blobs' of clusters seeming to have formed. This is nice to see as if this were not the case then that would raise suspisions on the clustering approach as having done anything much different to a random assignment.\n",
    "\n",
    "To illustrate this point a bit we will do the same chart as above but with a random clusrter assignemnt to illustrate the difference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T22:52:58.390503Z",
     "start_time": "2017-11-20T22:51:53.462921Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure() \n",
    "ax = fig.gca() \n",
    "\n",
    "# some things to handle the coloring\n",
    "jet = cm = plt.get_cmap('Paired') \n",
    "cNorm  = colors.Normalize(vmin=0, vmax=clusterer.labels_.max())\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)\n",
    "\n",
    "# go through each cell and color it by cluster\n",
    "for i in range(1,len(json_data.features)):\n",
    "    \n",
    "    # put a try block in in case can't find some cells in the cluster data for some reason.\n",
    "    \n",
    "    try:\n",
    "        poly = json_data.features[i]['geometry']\n",
    "        cell_id = str(json_data.features[i]['properties']['cellId'])\n",
    "        #cluster = df_final.loc[cell_id]['cluster']\n",
    "        cluster = np.random.choice(clusterer.labels_,1)[0] # randomly pick a cluster\n",
    "        colorVal = scalarMap.to_rgba(cluster)\n",
    "        \n",
    "        # add patch to the map\n",
    "        ax.add_patch(PolygonPatch(poly, fc=colorVal, ec=colorVal, alpha=0.8, zorder=1 ))\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ax.axis('scaled')\n",
    "\n",
    "fig.set_size_inches(11,11)\n",
    "plt.title(\"Cell Map Of Cluster Membership\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
